import os
import numpy as np
from typing import List, Dict, Any
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama.embeddings import OllamaEmbeddings

from src.config import settings
from src.store.pg_vector_store import PGVectorStore

# --- 1. ì„¤ì • (Configuration) ---
DB_URL = settings.SYNC_DATABASE_URL
DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
MODEL_NAME = settings.OLLAMA_EMBEDDING_MODEL_NAME
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200


def main():
    print("ğŸš€ Sentinel-Core ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ì‹œì‘...")

    # --- 2. ë°ì´í„° í´ë” í™•ì¸ ---
    if not os.path.exists(DATA_PATH):
        os.makedirs(DATA_PATH)
        print(
            f"'{DATA_PATH}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì¸ë±ì‹±í•  .md ë˜ëŠ” .txt íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”."
        )
        return

    # --- 3. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ---
    print(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘... ({DB_URL})")
    store = PGVectorStore(db_url=DB_URL)

    print(f"Ollama ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ëª¨ë¸: {MODEL_NAME})")
    # (Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤)
    embeddings = OllamaEmbeddings(model=MODEL_NAME)

    loader = DirectoryLoader(
        path=DATA_PATH,
        glob="**/*.md",
        show_progress=True,
        use_multithreading=True,
    )
    docs_md = loader.load()

    loader = DirectoryLoader(
        path=DATA_PATH,
        glob="**/*.txt",
        show_progress=True,
        use_multithreading=True,
    )
    docs_txt = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )

    # --- 4. (Phase 1) ë°ì´í„° ë¡œë“œ ---
    print(f"\n[Phase 1/4] '{DATA_PATH}'ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    docs = docs_md + docs_txt
    if not docs:
        print("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    print(f"ì´ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ.")

    # --- 5. (Phase 2) ë¬¸ì„œ ë¶„í•  (Split) ---
    print("[Phase 2/4] ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
    chunks = text_splitter.split_documents(docs)
    print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.")

    # --- 6. (Phase 3) ì„ë² ë”© ìƒì„± (Embed) ---
    print(
        f"[Phase 3/4] {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
    )
    chunk_texts = [chunk.page_content for chunk in chunks]

    # embed_documentsëŠ” List[List[float]]ì„ ë°˜í™˜
    chunk_embeddings_list = embeddings.embed_documents(chunk_texts)

    # pgvectorëŠ” numpy ë°°ì—´ì„ ì„ í˜¸í•¨
    chunk_embeddings_np = [np.array(emb) for emb in chunk_embeddings_list]
    print("ì„ë² ë”© ìƒì„± ì™„ë£Œ.")

    # --- 7. (Phase 4) DBì— ì ì¬ (Load) ---
    print("[Phase 4/4] DBì— ë°ì´í„° ì €ì¥(Upsert) ì¤‘...")

    # PGVectorStoreê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹(List[Dict])ìœ¼ë¡œ ë³€í™˜
    docs_to_store: List[Dict[str, Any]] = []
    for i, chunk in enumerate(chunks):
        doc_id = chunk.metadata.get("source", f"unknown-source-{i}")

        # PGVectorStoreì˜ upsert ë©”ì„œë“œê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ìŠµë‹ˆë‹¤.
        docs_to_store.append(
            {
                "doc_id": doc_id,
                "chunk_text": chunk.page_content,
                "embedding": chunk_embeddings_np[i],
                "metadata": chunk.metadata,
                # MVP: ëª¨ë“  íŒŒì¼ì€ 'file' íƒ€ì…ì´ê³  'all_users'ê°€ ë³¼ ìˆ˜ ìˆë‹¤ê³  ê°€ì •
                "source_type": "file",
                "permission_groups": ["all_users"],
            }
        )

    # (ì¤‘ìš”) PGVectorStoreì˜ `upsert_documents` ë©”ì„œë“œê°€ ì´ ë°ì´í„°ë¥¼
    # `documents`ì™€ `document_chunks` í…Œì´ë¸”ì— ë‚˜ëˆ  ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
    # *** ì•„ë˜ [3ë‹¨ê³„: PGVectorStore ì—…ë°ì´íŠ¸]ë¥¼ ê¼­ ë³´ì„¸ìš”. ***
    try:
        store.upsert_documents(docs_to_store)
        print("\nâœ… ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
    except Exception as e:
        print(f"\nâŒ ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
